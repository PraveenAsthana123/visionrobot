<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>04 Problem Statement Ipo - AR/VR Documentation</title>
    <script src="https://aframe.io/releases/1.5.0/aframe.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        body {
            margin: 0;
            overflow: hidden;
            font-family: 'Arial', sans-serif;
        }

        #overlay {
            position: fixed;
            top: 20px;
            left: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.8);
            color: white;
            padding: 30px;
            border-radius: 15px;
            max-height: 80vh;
            overflow-y: auto;
            z-index: 1000;
            backdrop-filter: blur(10px);
        }

        #overlay h1 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 2.5em;
        }

        #overlay h2 {
            color: #764ba2;
            margin-top: 30px;
            font-size: 1.8em;
        }

        #overlay code {
            background: rgba(102, 126, 234, 0.2);
            padding: 2px 6px;
            border-radius: 4px;
            color: #a5b4fc;
        }

        #overlay pre {
            background: rgba(0, 0, 0, 0.5);
            padding: 15px;
            border-radius: 8px;
            overflow-x: auto;
            border-left: 4px solid #667eea;
        }

        #overlay pre code {
            background: transparent;
        }

        #overlay table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        #overlay th {
            background: linear-gradient(135deg, #667eea, #764ba2);
            padding: 12px;
            text-align: left;
        }

        #overlay td {
            padding: 10px 12px;
            border-bottom: 1px solid rgba(255, 255, 255, 0.1);
        }

        #controls {
            position: fixed;
            bottom: 30px;
            left: 50%;
            transform: translateX(-50%);
            display: flex;
            gap: 15px;
            z-index: 1001;
        }

        .control-btn {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 50px;
            cursor: pointer;
            font-size: 1em;
            font-weight: 600;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
            transition: all 0.3s;
        }

        .control-btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }

        .info-panel {
            position: fixed;
            top: 20px;
            right: 20px;
            background: rgba(0, 0, 0, 0.8);
            color: white;
            padding: 20px;
            border-radius: 10px;
            z-index: 1000;
            backdrop-filter: blur(10px);
            max-width: 300px;
        }

        .info-panel h3 {
            color: #667eea;
            margin-bottom: 10px;
        }

        .info-panel p {
            margin: 5px 0;
            font-size: 0.9em;
        }
    </style>
</head>
<body>
    <div id="overlay" style="display: none;">
        <div id="content"></div>
    </div>

    <div class="info-panel">
        <h3>🥽 VR Documentation</h3>
        <p><strong>04 Problem Statement Ipo</strong></p>
        <p>Use WASD to move</p>
        <p>Mouse to look around</p>
        <p>Click 'Toggle Doc' to read</p>
    </div>

    <div id="controls">
        <button class="control-btn" onclick="toggleOverlay()">📄 Toggle Doc</button>
        <button class="control-btn" onclick="enterVR()">🥽 Enter VR</button>
        <button class="control-btn" onclick="resetView()">🔄 Reset View</button>
    </div>

    <a-scene background="color: #000">
        <!-- Sky with gradient -->
        <a-sky color="#1a1a2e"></a-sky>

        <!-- Ambient lighting -->
        <a-light type="ambient" color="#667eea" intensity="0.5"></a-light>
        <a-light type="directional" color="#FFF" intensity="0.8" position="2 4 -3"></a-light>
        <a-light type="point" color="#764ba2" intensity="1" position="0 3 0"></a-light>

        <!-- Floor grid -->
        <a-plane position="0 0 0" rotation="-90 0 0" width="20" height="20"
                 color="#0f0f23" roughness="0.8" metalness="0.2"
                 shader="flat" repeat="10 10"></a-plane>

        <!-- Documentation panels floating in 3D space -->
        <a-entity id="doc-environment" position="0 1.6 -3">
            <!-- Main title panel -->
            <a-plane position="0 2 0" rotation="0 0 0" width="8" height="1.5"
                     color="#667eea" opacity="0.9">
                <a-text value="04 Problem Statement Ipo" align="center" color="#FFF"
                        position="0 0 0.01" width="8" font="roboto"></a-text>
            </a-plane>

            <!-- Floating data panels in circular arrangement -->
            <a-entity id="panel-1" position="-3 0 0">
                <a-box color="#667eea" opacity="0.7" width="1.5" height="2" depth="0.1"
                       roughness="0.3" metalness="0.6"
                       animation="property: rotation; to: 0 360 0; loop: true; dur: 20000">
                    <a-text value="PDF\nFormat" align="center" color="#FFF"
                            position="0 0.5 0.06" width="2"></a-text>
                    <a-text value="Complete\nDocumentation" align="center" color="#CCC"
                            position="0 -0.3 0.06" width="1.5"></a-text>
                </a-box>
            </a-entity>

            <a-entity id="panel-2" position="3 0 0">
                <a-box color="#764ba2" opacity="0.7" width="1.5" height="2" depth="0.1"
                       roughness="0.3" metalness="0.6"
                       animation="property: rotation; to: 0 360 0; loop: true; dur: 20000; dir: reverse">
                    <a-text value="Interactive\n3D View" align="center" color="#FFF"
                            position="0 0.5 0.06" width="2"></a-text>
                    <a-text value="Immersive\nExperience" align="center" color="#CCC"
                            position="0 -0.3 0.06" width="1.5"></a-text>
                </a-box>
            </a-entity>

            <a-entity id="panel-3" position="0 0 -2">
                <a-cylinder color="#f093fb" opacity="0.7" radius="0.75" height="2"
                           roughness="0.3" metalness="0.6"
                           animation="property: position; to: 0 0.5 -2; loop: true; dur: 3000; dir: alternate; easing: easeInOutSine">
                    <a-text value="VisionBot\nProject" align="center" color="#FFF"
                            position="0 0.5 0.76" width="2"></a-text>
                </a-cylinder>
            </a-entity>

            <!-- Floating stats spheres -->
            <a-sphere position="-1.5 1.5 -1" radius="0.3" color="#10b981" opacity="0.8"
                     animation="property: position; to: -1.5 2 -1; loop: true; dur: 2000; dir: alternate">
                <a-text value="99.2%\nSuccess" align="center" color="#FFF"
                        position="0 0 0.31" width="1.2"></a-text>
            </a-sphere>

            <a-sphere position="1.5 1.5 -1" radius="0.3" color="#f59e0b" opacity="0.8"
                     animation="property: position; to: 1.5 2 -1; loop: true; dur: 2500; dir: alternate">
                <a-text value="93.5%\nOEE" align="center" color="#FFF"
                        position="0 0 0.31" width="1.2"></a-text>
            </a-sphere>
        </a-entity>

        <!-- Camera with controls -->
        <a-entity id="camera-rig" position="0 0 0">
            <a-camera id="main-camera" wasd-controls look-controls>
                <a-cursor color="#667eea" fuse="true" fuse-timeout="1500"></a-cursor>
            </a-camera>
        </a-entity>

        <!-- Particle effect -->
        <a-entity position="0 2.5 -5">
            <a-entity geometry="primitive: sphere; radius: 0.05"
                     material="color: #667eea; emissive: #667eea; emissiveIntensity: 0.5"
                     animation="property: position; to: 0 4 -5; loop: true; dur: 4000; easing: linear"></a-entity>
        </a-entity>
    </a-scene>

    <script>
        // Convert markdown to HTML
        const markdown = `# Problem Statement + IPO Analysis - Vision-Based Pick and Place System

## 1. Problem Statement

### 1.1 Business Problem
**Context:**
Manufacturing, warehousing, and logistics industries face challenges with:
- **Labor-intensive** manual pick-and-place operations
- **High error rates** in object sorting and placement
- **Scalability limitations** due to workforce constraints
- **Repetitive strain injuries** from repetitive manual tasks
- **Inconsistent throughput** due to human fatigue
- **Inability to operate 24/7** without shift rotations

**Opportunity:**
Automate object detection, grasping, and placement using vision-guided robotics to:
- Increase throughput (target: 30 picks/minute)
- Reduce errors (target: <1% failure rate)
- Operate continuously (24/7 with minimal supervision)
- Handle varying object types/sizes (within defined workspace)
- Ensure worker safety (collaborative operation)

### 1.2 Technical Problem
**Challenge:**
Develop a robotic system that can:
1. **Perceive:** Detect and localize objects in a cluttered workspace
2. **Plan:** Compute collision-free trajectories to pick and place objects
3. **Execute:** Precisely grasp objects and place them at target locations
4. **Adapt:** Handle variations in object pose, lighting, occlusions
5. **Ensure Safety:** Operate safely near humans, detect collisions

**Constraints:**
- **Real-time:** Vision processing <50ms, control loop 1kHz
- **Accuracy:** Position repeatability ±0.1mm
- **Reliability:** 99.9% uptime, graceful error recovery
- **Cost:** Solution must be cost-effective (ROI <2 years)

### 1.3 Success Criteria
| **Metric**                  | **Target**         | **Measurement Method**              |
|-----------------------------|--------------------|-------------------------------------|
| Pick rate                   | ≥30 picks/min      | Throughput test (100 cycles)        |
| Grasp success rate          | ≥99%               | 1000-pick test, count failures      |
| Positional accuracy         | ±0.1mm             | CMM measurement at target location  |
| Vision detection accuracy   | ≥95% mAP           | Test on labeled dataset             |
| Cycle time                  | ≤2 sec/object      | Time from scan to place completion  |
| Uptime                      | ≥99.5%             | Track operational hours vs downtime |
| Safety incidents            | 0                  | Collision detection, E-stop tests   |

---

## 2. System-Level IPO (Input-Process-Output)

### 2.1 High-Level System IPO

\`\`\`
┌─────────────────────────────────────────────────────────────────┐
│                         SYSTEM BOUNDARY                         │
│                                                                 │
│  INPUTS                  PROCESS                   OUTPUTS      │
│  ─────────              ──────────                ─────────     │
│                                                                 │
│  • Objects in            ┌──────────────┐         • Objects at  │
│    workspace             │   VISION     │           target      │
│  • Target locations      │  PERCEPTION  │           locations   │
│  • User commands         └──────┬───────┘         • Status      │
│    (start/stop)                 │                   reports     │
│  • Environmental                ▼                 • Logs/       │
│    data (lighting,       ┌──────────────┐           telemetry  │
│    obstacles)            │   MOTION     │         • Alerts/     │
│                          │   PLANNING   │           alarms      │
│                          └──────┬───────┘                       │
│                                 │                               │
│                                 ▼                               │
│                          ┌──────────────┐                       │
│                          │  EXECUTION   │                       │
│                          │  & CONTROL   │                       │
│                          └──────────────┘                       │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
\`\`\`

### 2.2 Detailed System IPO Table

| **Phase**  | **Input**                          | **Process**                                   | **Output**                          |
|------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **1. Scan**| - RGB-D camera stream<br>- Trigger command | - Capture image<br>- Preprocess (denoise, crop) | - RGB image (1920x1080)<br>- Depth map |
| **2. Detect**| - RGB image<br>- Pre-trained model | - Run object detection (YOLO)<br>- Filter low-confidence detections | - Bounding boxes<br>- Class labels<br>- Confidence scores |
| **3. Localize**| - Bounding boxes<br>- Depth map<br>- Camera calibration | - Extract 3D point cloud<br>- Estimate 6DoF pose<br>- Transform to robot frame | - Object pose (x,y,z,r,p,y)<br>- Point cloud |
| **4. Plan Grasp**| - Object pose<br>- Point cloud<br>- Gripper constraints | - Compute grasp candidates<br>- Rank by quality score<br>- Select best grasp | - Gripper pose<br>- Approach vector<br>- Grasp quality score |
| **5. Plan Motion**| - Gripper pose<br>- Current robot state<br>- Obstacles | - Inverse kinematics<br>- Path planning (RRT*)<br>- Trajectory generation | - Joint trajectory (waypoints)<br>- Collision-free path |
| **6. Execute Pick**| - Joint trajectory<br>- Gripper command | - Send trajectory to controller<br>- Monitor execution<br>- Close gripper | - Joint positions (actual)<br>- Grasp force feedback |
| **7. Plan Place**| - Target location<br>- Current robot state | - Compute placement pose<br>- Plan trajectory (pick → place) | - Placement trajectory<br>- Orientation |
| **8. Execute Place**| - Placement trajectory<br>- Gripper release command | - Move to target<br>- Open gripper<br>- Retract | - Object at target location<br>- Task completion status |
| **9. Verify**| - Camera image<br>- Expected location | - Capture post-placement image<br>- Verify object position | - Success/failure flag<br>- Error magnitude |

---

## 3. Module-Level IPO

### 3.1 Vision Perception Module

**Purpose:** Detect and localize objects in 3D space

| **Component**         | **Input**                          | **Process**                                   | **Output**                          |
|-----------------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **Image Acquisition** | - Camera trigger<br>- Exposure settings | - Capture RGB-D frame<br>- Sync RGB and depth | - RGB image<br>- Aligned depth map |
| **Preprocessing**     | - Raw RGB image                    | - Resize to 640x640<br>- Normalize pixel values | - Preprocessed tensor (3x640x640) |
| **Object Detection**  | - Preprocessed image               | - Forward pass through YOLOv8<br>- NMS (non-max suppression) | - Bounding boxes [x,y,w,h]<br>- Class IDs<br>- Confidence scores |
| **Pose Estimation**   | - RGB-D image<br>- Bounding box<br>- Object model | - Extract object ROI<br>- Run PnP or deep pose estimator<br>- Refine with ICP | - 6DoF pose [x,y,z,qx,qy,qz,qw]<br>- Covariance (uncertainty) |
| **Point Cloud Gen.**  | - Depth map<br>- Camera intrinsics | - Deproject pixels to 3D points<br>- Filter outliers (statistical) | - Point cloud (XYZ + RGB)<br>- Downsampled cloud |
| **Coordinate Transform**| - Object pose (camera frame)<br>- TF tree (camera→robot) | - Apply homogeneous transformation<br>- Publish TF | - Object pose (robot base frame) |

**IPO Summary:**
\`\`\`
INPUT: RGB-D frames (30 Hz)
PROCESS: Detection → Pose Estimation → Coordinate Transform
OUTPUT: Object poses in robot frame (x,y,z,roll,pitch,yaw) at 10 Hz
\`\`\`

---

### 3.2 Grasp Planning Module

**Purpose:** Compute optimal gripper pose for stable grasping

| **Component**         | **Input**                          | **Process**                                   | **Output**                          |
|-----------------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **Grasp Candidate Gen.**| - Object point cloud<br>- Gripper geometry | - Sample grasp poses (centroid, normals)<br>- Check reachability | - List of candidate grasps [pose, score] |
| **Collision Check**   | - Grasp candidates<br>- Scene point cloud | - Check gripper-object collision<br>- Check gripper-table collision | - Collision-free grasps |
| **Grasp Ranking**     | - Grasp candidates<br>- Force closure metrics | - Compute grasp quality (wrench space)<br>- Sort by score | - Ranked list of grasps |
| **Grasp Selection**   | - Ranked grasps                    | - Select top-ranked grasp<br>- Fallback to 2nd if 1st fails | - Selected grasp pose<br>- Approach vector |

**IPO Summary:**
\`\`\`
INPUT: Object pose, point cloud
PROCESS: Sample grasps → Filter collisions → Rank by quality → Select best
OUTPUT: Gripper pose (6DoF) + approach direction
\`\`\`

---

### 3.3 Motion Planning Module (MoveIt2)

**Purpose:** Generate collision-free trajectories

| **Component**         | **Input**                          | **Process**                                   | **Output**                          |
|-----------------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **Planning Scene**    | - Robot URDF<br>- Point cloud (obstacles) | - Build occupancy grid<br>- Update collision objects | - Planning scene (internal state) |
| **IK Solver**         | - Target end-effector pose<br>- Current joint state | - Solve inverse kinematics<br>- Validate joint limits | - Joint angles [θ1..θ6]<br>- IK success flag |
| **Path Planner (OMPL)**| - Start state<br>- Goal state<br>- Planning scene | - Run RRT*/PRM<br>- Search for collision-free path | - Path (sequence of joint configs) |
| **Trajectory Generator**| - Path waypoints<br>- Velocity/accel limits | - Time-parameterization (parabolic blend)<br>- Smooth jerk | - Joint trajectory (time-stamped) |
| **Trajectory Smoothing**| - Raw trajectory                | - Iterative optimization (shortcut, smooth) | - Smoothed trajectory |

**IPO Summary:**
\`\`\`
INPUT: Target pose (x,y,z,roll,pitch,yaw), obstacles
PROCESS: IK → Path Planning (RRT*) → Trajectory Generation → Smoothing
OUTPUT: Time-parameterized joint trajectory
\`\`\`

---

### 3.4 Control & Execution Module (ros2_control)

**Purpose:** Execute trajectories with real-time feedback control

| **Component**         | **Input**                          | **Process**                                   | **Output**                          |
|-----------------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **Trajectory Interpolator**| - Joint trajectory<br>- Current time | - Interpolate setpoints at control frequency (1kHz) | - Joint position/velocity setpoints |
| **Joint Controller (PID)**| - Setpoint<br>- Actual position (encoder) | - Compute error<br>- PID control law<br>- Feedforward compensation | - Motor torque command |
| **Motor Driver Interface**| - Torque command              | - Convert to current command<br>- Send via EtherCAT | - Motor current (3-phase) |
| **Feedback Loop**     | - Encoder position/velocity        | - Read encoder data<br>- Publish joint states | - Joint states (position, velocity, effort) |
| **Safety Monitor**    | - Following error<br>- Joint limits | - Check error bounds<br>- Detect collisions (F/T sensor) | - Safety status<br>- E-stop trigger |

**IPO Summary:**
\`\`\`
INPUT: Joint trajectory
PROCESS: Interpolate → PID Control → Motor Drive → Feedback
OUTPUT: Robot motion (joint positions), safety status
\`\`\`

---

### 3.5 Gripper Control Module

**Purpose:** Actuate gripper (open/close) with force control

| **Component**         | **Input**                          | **Process**                                   | **Output**                          |
|-----------------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **Gripper Controller**| - Gripper command (open/close/force) | - Compute motor PWM<br>- Apply force setpoint | - Gripper motor PWM signal |
| **Force Feedback**    | - F/T sensor readings              | - Measure grip force<br>- Compare to setpoint | - Actual grip force<br>- Force error |
| **Position Feedback** | - Encoder/limit switches           | - Measure jaw opening<br>- Detect object presence | - Jaw position<br>- Grasp success flag |

**IPO Summary:**
\`\`\`
INPUT: Gripper command (open/close), target force
PROCESS: Force control loop (PID)
OUTPUT: Gripper state (open/closed), grip force
\`\`\`

---

### 3.6 Task Orchestration Module (State Machine)

**Purpose:** High-level task sequencing and error recovery

| **Component**         | **Input**                          | **Process**                                   | **Output**                          |
|-----------------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **State Machine**     | - System events (triggers)<br>- Sensor data | - Evaluate state transitions<br>- Execute state actions | - Current state<br>- State outputs |
| **Error Handler**     | - Fault signals (vision fail, grasp fail) | - Analyze failure type<br>- Trigger recovery action | - Recovery command<br>- Retry/abort decision |
| **Task Scheduler**    | - Task queue<br>- Robot availability | - Prioritize tasks<br>- Dispatch to modules | - Task assignments<br>- Execution order |

**States:**
1. **IDLE:** Wait for start command
2. **SCAN:** Capture image
3. **DETECT:** Run vision pipeline
4. **PLAN_PICK:** Compute grasp and trajectory
5. **EXECUTE_PICK:** Move and grasp
6. **PLAN_PLACE:** Compute placement trajectory
7. **EXECUTE_PLACE:** Move and release
8. **VERIFY:** Check placement success
9. **ERROR:** Handle failures, retry or abort
10. **HOME:** Return to home position

**IPO Summary:**
\`\`\`
INPUT: User commands, sensor events
PROCESS: State transitions based on events and conditions
OUTPUT: High-level commands to subsystems (vision, motion, gripper)
\`\`\`

---

### 3.7 Monitoring & Logging Module

**Purpose:** Collect telemetry, logs, and metrics for observability

| **Component**         | **Input**                          | **Process**                                   | **Output**                          |
|-----------------------|------------------------------------|-----------------------------------------------|-------------------------------------|
| **Data Collector**    | - ROS topics (joint states, images, etc.) | - Subscribe to topics<br>- Timestamp data | - Time-series data streams |
| **Logger**            | - Log messages (INFO, WARN, ERROR) | - Format logs<br>- Write to file/database | - Log files<br>- Database entries |
| **Metrics Aggregator**| - Performance metrics (latency, success rate) | - Compute statistics (mean, std, percentiles) | - Aggregated metrics |
| **Alert Manager**     | - Metric thresholds<br>- Anomalies | - Evaluate alert rules<br>- Trigger notifications | - Alerts (email, SMS, dashboard) |
| **Visualization**     | - Time-series data<br>- Logs        | - Render graphs (Grafana)<br>- Display logs (Kibana) | - Dashboards<br>- Real-time plots |

**IPO Summary:**
\`\`\`
INPUT: ROS topics, log messages, performance metrics
PROCESS: Collect → Store → Aggregate → Visualize
OUTPUT: Dashboards, alerts, historical logs
\`\`\`

---

## 4. Data Flow Diagram

### 4.1 End-to-End Data Flow

\`\`\`
┌────────────┐
│   Camera   │ (RGB-D frames @ 30 Hz)
└──────┬─────┘
       │
       ▼
┌─────────────────┐
│ Vision Pipeline │ (Object poses @ 10 Hz)
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│ Grasp Planner   │ (Gripper pose)
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│ Motion Planner  │ (Joint trajectory)
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│ ros2_control    │ (Motor commands @ 1 kHz)
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│  Motor Drivers  │ (Currents to motors)
└──────┬──────────┘
       │
       ▼
┌─────────────────┐
│ Robot Actuators │ (Physical motion)
└──────┬──────────┘
       │
       ▼ (feedback)
┌─────────────────┐
│    Encoders     │ (Joint positions @ 1 kHz)
└──────┬──────────┘
       │
       └─────► (feedback to ros2_control)
\`\`\`

---

## 5. IPO for Key Interfaces

### 5.1 Camera ↔ Vision Pipeline

| **Aspect**  | **Details**                                                     |
|-------------|-----------------------------------------------------------------|
| **Input**   | USB 3.0 stream (RGB 1920x1080 @ 30fps, Depth 1280x720 @ 30fps) |
| **Process** | Image transport (compressed), synchronization (ApproxTime)      |
| **Output**  | ROS2 topics: \`/camera/color/image_raw\`, \`/camera/depth/image_rect\` |
| **Latency** | <30ms (USB transfer + decompression)                            |
| **Data Rate**| ~200 MB/s (uncompressed), ~50 MB/s (compressed JPEG)            |

### 5.2 Vision Pipeline ↔ Motion Planning

| **Aspect**  | **Details**                                                     |
|-------------|-----------------------------------------------------------------|
| **Input**   | Object pose (geometry_msgs/PoseStamped)                         |
| **Process** | ROS2 service call: \`/compute_grasp\` → \`/plan_pick_motion\`      |
| **Output**  | Joint trajectory (trajectory_msgs/JointTrajectory)              |
| **Latency** | 200-500ms (IK + planning)                                       |
| **Frequency**| On-demand (per object detected)                                |

### 5.3 Motion Planning ↔ Control

| **Aspect**  | **Details**                                                     |
|-------------|-----------------------------------------------------------------|
| **Input**   | Joint trajectory (moveit_msgs/action/ExecuteTrajectory)         |
| **Process** | ROS2 action interface (goal, feedback, result)                  |
| **Output**  | Joint commands @ 1kHz (control_msgs/JointTrajectoryControllerState) |
| **Latency** | <10ms (action call overhead)                                    |
| **Frequency**| 1 kHz (controller loop)                                         |

### 5.4 Control ↔ Motor Drivers

| **Aspect**  | **Details**                                                     |
|-------------|-----------------------------------------------------------------|
| **Input**   | Motor current commands (EtherCAT PDO, 16-bit signed int)        |
| **Process** | EtherCAT cyclic communication (1 kHz)                           |
| **Output**  | Motor phase currents (Ia, Ib, Ic)                               |
| **Latency** | <1ms (deterministic EtherCAT cycle)                             |
| **Jitter**  | <10 μs (EtherCAT distributed clocks)                            |

---

## 6. Performance Requirements per Module

| **Module**              | **Throughput**        | **Latency**         | **Accuracy**          | **Reliability** |
|-------------------------|-----------------------|---------------------|-----------------------|-----------------|
| Vision Perception       | 10 detections/sec     | <50ms per frame     | mAP ≥0.95             | 99% uptime      |
| Pose Estimation         | 10 poses/sec          | <100ms per object   | ±5mm position, ±5° orientation | 95% accuracy |
| Grasp Planning          | 5 grasps/sec          | <200ms per object   | Quality score ≥0.8    | 90% success     |
| Motion Planning         | 2 plans/sec           | <500ms per plan     | Collision-free (100%) | 99% plan success|
| Trajectory Execution    | 1 kHz control loop    | <10ms per cycle     | Tracking error <2mm   | 99.9% uptime    |
| Gripper Control         | 100 Hz                | <10ms               | Force ±1N             | 99% grasp success|
| Task Orchestration      | 1 task/2sec           | <50ms state trans.  | N/A                   | 100% state integrity|

---

## 7. Error Handling IPO

### 7.1 Error Detection Inputs

| **Error Type**          | **Input Signal**                                      | **Detection Method**              |
|-------------------------|-------------------------------------------------------|-----------------------------------|
| Vision failure          | No objects detected for >5 sec                        | Timeout counter                   |
| Grasp failure           | F/T sensor: grip force <threshold                     | Force threshold check             |
| Motion planning failure | Planner returns no solution                           | Planner status code               |
| Collision               | F/T sensor: force spike >150N                         | Anomaly detection (force)         |
| Joint limit violation   | Encoder position outside [q_min, q_max]               | Range check                       |
| E-stop                  | Emergency stop button pressed                         | Digital input (hardwired)         |

### 7.2 Error Recovery Process

| **Error Type**          | **Recovery Action**                                   | **Outcome**                       |
|-------------------------|-------------------------------------------------------|-----------------------------------|
| Vision failure          | Re-trigger camera, adjust lighting, rescan            | Resume detection or abort         |
| Grasp failure           | Retry with alternate grasp, reduce speed              | Retry (max 3x) or skip object     |
| Motion planning failure | Relax constraints, replan with wider clearance        | New plan or abort task            |
| Collision               | E-stop, retract, re-home robot                        | Safe state, await manual reset    |
| Joint limit violation   | Stop motion, move back to safe position               | Resume from safe configuration    |
| E-stop                  | Power off motors, log event, await user reset         | System halted, manual intervention|

---

## 8. IPO Summary Matrix

| **Module**              | **Input**                          | **Process**                        | **Output**                         | **Frequency** |
|-------------------------|------------------------------------|------------------------------------|------------------------------------|---------------|
| Vision Perception       | RGB-D frames                       | Detection + Pose Estimation        | Object poses (robot frame)         | 10 Hz         |
| Grasp Planning          | Object pose, point cloud           | Sample + Rank grasps               | Gripper pose, quality score        | On-demand     |
| Motion Planning         | Target pose, obstacles             | IK + Path Planning                 | Joint trajectory                   | On-demand     |
| Trajectory Execution    | Joint trajectory                   | Interpolation + PID control        | Motor commands                     | 1 kHz         |
| Gripper Control         | Gripper command, force setpoint    | Force control loop                 | Gripper state, grip force          | 100 Hz        |
| Task Orchestration      | User commands, sensor events       | State machine transitions          | High-level commands to modules     | Event-driven  |
| Monitoring & Logging    | ROS topics, logs, metrics          | Collect + Aggregate + Visualize    | Dashboards, alerts                 | Continuous    |

---

## 9. Dimensional Analysis

### 9.1 Data Dimensions

| **Data Type**           | **Dimensions**                     | **Units**                          | **Example Value**                 |
|-------------------------|------------------------------------|------------------------------------|------------------------------------|
| RGB Image               | 1920 × 1080 × 3                    | pixels (uint8)                     | [0-255] per channel                |
| Depth Image             | 1280 × 720                         | mm (uint16)                        | 0-10000 mm                         |
| Point Cloud             | N × 3 (XYZ)                        | m (float32)                        | N ≈ 100,000 points                 |
| Object Pose             | 7 (x,y,z,qx,qy,qz,qw)              | m, quaternion (float64)            | [0.5, 0.2, 0.1, 0,0,0,1]           |
| Joint Angles            | 6 (θ1..θ6)                         | rad (float64)                      | [-π, π]                            |
| Joint Trajectory        | T × 6 (time, positions)            | s, rad                             | T ≈ 100 waypoints                  |
| Force/Torque            | 6 (Fx,Fy,Fz,Tx,Ty,Tz)              | N, N·m (float64)                   | Fx ∈ [-100, 100] N                 |
| Gripper State           | 2 (position, force)                | m, N                               | [0.05m, 20N]                       |

---

## 10. Conclusion

This IPO documentation provides a **complete mapping** of:
- **System-level** inputs, processes, and outputs
- **Module-level** IPO for each major subsystem
- **Data flow** through the entire pipeline (camera → motors)
- **Interfaces** between modules with latency and data rate specs
- **Error handling** IPO for fault detection and recovery

**Key Takeaways:**
- **Vision → Planning → Control** pipeline with clear IPO at each stage
- **Real-time performance** requirements (1 kHz control, <50ms vision)
- **Dimensional consistency** enforced across all data types
- **Error recovery** mechanisms for robust operation

---

**Document Status:** ✅ Complete
**Last Updated:** 2025-10-18
**Author:** System Engineering Team
**Review Status:** Pending Review
`;

        document.addEventListener('DOMContentLoaded', function() {
            const contentDiv = document.getElementById('content');
            contentDiv.innerHTML = marked.parse(markdown);
        });

        function toggleOverlay() {
            const overlay = document.getElementById('overlay');
            overlay.style.display = overlay.style.display === 'none' ? 'block' : 'none';
        }

        function enterVR() {
            const scene = document.querySelector('a-scene');
            if (scene.is('vr-mode')) {
                scene.exitVR();
            } else {
                scene.enterVR();
            }
        }

        function resetView() {
            const cameraRig = document.getElementById('camera-rig');
            cameraRig.setAttribute('position', '0 0 0');
            const camera = document.getElementById('main-camera');
            camera.setAttribute('rotation', '0 0 0');
        }

        // Keyboard shortcuts
        document.addEventListener('keydown', function(e) {
            if (e.key === 'd' || e.key === 'D') {
                toggleOverlay();
            } else if (e.key === 'r' || e.key === 'R') {
                resetView();
            }
        });
    </script>
</body>
</html>