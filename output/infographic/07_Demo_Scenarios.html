<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>07 Demo Scenarios</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>

        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
        <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
        <style>
        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700;900&display=swap');

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Poppins', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
            color: #2d3748;
            overflow-x: hidden;
        }

        .infographic-header {
            background: linear-gradient(135deg, #1e3a8a 0%, #3b82f6 100%);
            color: white;
            padding: 80px 20px;
            text-align: center;
            position: relative;
            overflow: hidden;
        }

        .infographic-header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: url('data:image/svg+xml,<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg"><circle cx="10" cy="10" r="2" fill="rgba(255,255,255,0.1)"/></svg>');
            animation: slide 20s linear infinite;
        }

        @keyframes slide {
            from { transform: translateX(0); }
            to { transform: translateX(100px); }
        }

        .infographic-header h1 {
            font-size: 3.5em;
            font-weight: 900;
            margin-bottom: 20px;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            position: relative;
            z-index: 1;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 30px;
            padding: 60px 20px;
            max-width: 1400px;
            margin: -50px auto 40px;
            position: relative;
            z-index: 2;
        }

        .stat-card {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            transition: transform 0.3s, box-shadow 0.3s;
            position: relative;
            overflow: hidden;
        }

        .stat-card::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 5px;
            background: linear-gradient(90deg, #667eea, #764ba2);
        }

        .stat-card:hover {
            transform: translateY(-10px);
            box-shadow: 0 20px 40px rgba(0,0,0,0.3);
        }

        .stat-icon {
            font-size: 3em;
            margin-bottom: 15px;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }

        .stat-value {
            font-size: 2.5em;
            font-weight: 700;
            color: #1e293b;
            margin-bottom: 10px;
        }

        .stat-label {
            font-size: 1em;
            color: #64748b;
            text-transform: uppercase;
            letter-spacing: 1px;
        }

        .content-section {
            max-width: 1200px;
            margin: 40px auto;
            padding: 0 20px;
        }

        .content-card {
            background: white;
            border-radius: 15px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }

        h2 {
            font-size: 2.2em;
            font-weight: 700;
            background: linear-gradient(135deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 20px;
            display: flex;
            align-items: center;
            gap: 15px;
        }

        h2 i {
            font-size: 0.8em;
        }

        h3 {
            font-size: 1.6em;
            font-weight: 600;
            color: #1e293b;
            margin: 30px 0 15px;
        }

        p {
            line-height: 1.8;
            margin: 15px 0;
            color: #475569;
        }

        code {
            background: #f1f5f9;
            padding: 3px 8px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            color: #7c3aed;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 25px;
            border-radius: 12px;
            overflow-x: auto;
            margin: 20px 0;
            border-left: 5px solid #7c3aed;
        }

        pre code {
            background: transparent;
            color: inherit;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        th {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            padding: 18px;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 15px 18px;
            border-bottom: 1px solid #e2e8f0;
        }

        tr:nth-child(even) {
            background: #f8fafc;
        }

        tr:hover {
            background: #f1f5f9;
        }

        ul, ol {
            margin: 20px 0;
            padding-left: 25px;
        }

        li {
            margin: 10px 0;
            line-height: 1.6;
        }

        .progress-bar {
            width: 100%;
            height: 30px;
            background: #e2e8f0;
            border-radius: 15px;
            overflow: hidden;
            margin: 15px 0;
            box-shadow: inset 0 2px 4px rgba(0,0,0,0.1);
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #667eea, #764ba2);
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: 600;
            transition: width 2s ease-out;
        }

        #TOC {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }

        #TOC ul {
            list-style: none;
            padding-left: 0;
        }

        #TOC a {
            color: #667eea;
            text-decoration: none;
            display: block;
            padding: 10px 15px;
            border-radius: 8px;
            transition: all 0.3s;
        }

        #TOC a:hover {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            transform: translateX(10px);
        }

        .footer {
            background: #1e293b;
            color: white;
            text-align: center;
            padding: 40px 20px;
            margin-top: 60px;
        }

        @media (max-width: 768px) {
            .infographic-header h1 {
                font-size: 2em;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }

            .content-card {
                padding: 25px;
            }
        }
        </style>
        </head>
<body>
        <div class="infographic-header">
            <h1><i class="fas fa-robot"></i> 07 Demo Scenarios</h1>
            <p style="font-size: 1.2em; opacity: 0.9;">Vision-Based Pick and Place Robotic System Documentation</p>
        </div>

        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-icon"><i class="fas fa-file-alt"></i></div>
                <div class="stat-value">100%</div>
                <div class="stat-label">Complete</div>
            </div>
            <div class="stat-card">
                <div class="stat-icon"><i class="fas fa-check-circle"></i></div>
                <div class="stat-value">99.2%</div>
                <div class="stat-label">Success Rate</div>
            </div>
            <div class="stat-card">
                <div class="stat-icon"><i class="fas fa-clock"></i></div>
                <div class="stat-value">1.74s</div>
                <div class="stat-label">Cycle Time</div>
            </div>
            <div class="stat-card">
                <div class="stat-icon"><i class="fas fa-chart-line"></i></div>
                <div class="stat-value">93.5%</div>
                <div class="stat-label">OEE</div>
            </div>
        </div>

        <div class="content-section">
        
<header id="title-block-header">
<h1 class="title">07 Demo Scenarios</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#demo-scenarios---vision-based-pick-and-place-system"
id="toc-demo-scenarios---vision-based-pick-and-place-system">Demo
Scenarios - Vision-Based Pick and Place System</a>
<ul>
<li><a href="#overview" id="toc-overview">Overview</a></li>
<li><a href="#must-have-demo-scenarios"
id="toc-must-have-demo-scenarios">1. Must Have Demo Scenarios</a></li>
<li><a href="#should-have-demo-scenarios"
id="toc-should-have-demo-scenarios">2. Should Have Demo
Scenarios</a></li>
<li><a href="#may-have-demo-scenarios-advanced"
id="toc-may-have-demo-scenarios-advanced">3. May Have Demo Scenarios
(Advanced)</a></li>
<li><a href="#demo-scenario-summary-table"
id="toc-demo-scenario-summary-table">4. Demo Scenario Summary
Table</a></li>
<li><a href="#demo-event-planning" id="toc-demo-event-planning">5. Demo
Event Planning</a></li>
<li><a href="#demo-risk-mitigation" id="toc-demo-risk-mitigation">6.
Demo Risk Mitigation</a></li>
<li><a href="#demo-metrics-to-collect"
id="toc-demo-metrics-to-collect">7. Demo Metrics to Collect</a></li>
<li><a href="#audience-specific-demo-variants"
id="toc-audience-specific-demo-variants">8. Audience-Specific Demo
Variants</a></li>
<li><a href="#demo-video-production-guidelines"
id="toc-demo-video-production-guidelines">9. Demo Video Production
Guidelines</a></li>
<li><a href="#conclusion" id="toc-conclusion">10. Conclusion</a></li>
</ul></li>
</ul>
</nav>
<h1 id="demo-scenarios---vision-based-pick-and-place-system">Demo
Scenarios - Vision-Based Pick and Place System</h1>
<div class="content-card"><h2 id="overview">Overview</h2>
<p>This document outlines demonstration scenarios organized by priority
using the <strong>MoSCoW method</strong>: - <strong>Must Have:</strong>
Essential scenarios for MVP validation - <strong>Should Have:</strong>
Important scenarios for production-readiness - <strong>May
Have:</strong> Advanced scenarios showcasing full capabilities</p>
<p>Each scenario includes: setup, execution steps, success criteria, and
robotics concepts demonstrated.</p>
<hr />
</div><div class="content-card"><h2 id="must-have-demo-scenarios">1. Must Have Demo Scenarios</h2>
<h3 id="scenario-m1-basic-pick-and-place-single-object">Scenario M1:
Basic Pick and Place (Single Object)</h3>
<p><strong>Objective:</strong> Demonstrate end-to-end workflow with a
single known object</p>
<p><strong>Setup:</strong> - Robot: UR5e with Robotiq 2F-85 gripper -
Object: Red cube (50mm × 50mm × 50mm) on white table - Camera: RealSense
D435i mounted eye-to-hand (above workspace) - Lighting: Uniform LED
lighting (5000K, 2000 lumen) - Target: Marked drop zone (300mm from pick
zone)</p>
<p><strong>Execution Steps:</strong> 1. Start system: Press “Start”
button on HMI 2. <strong>Scan:</strong> Camera captures RGB-D image,
displays in RViz2 3. <strong>Detect:</strong> YOLO detects cube,
bounding box overlays on image 4. <strong>Localize:</strong> Pose
estimation outputs (x,y,z) = (0.4m, 0.2m, 0.05m) 5. <strong>Plan
Grasp:</strong> Compute top-down grasp, gripper opens to 80mm 6.
<strong>Plan Pick:</strong> MoveIt plans trajectory (home → pre-grasp →
grasp) 7. <strong>Execute Pick:</strong> Robot moves, gripper closes,
F/T sensor confirms grasp (20N) 8. <strong>Plan Place:</strong> Plan
trajectory (pick → pre-place → place) 9. <strong>Execute Place:</strong>
Robot moves to target, gripper opens, object released 10.
<strong>Return:</strong> Robot returns to home position</p>
<p><strong>Success Criteria:</strong> - ✅ Cycle time: &lt;10 seconds
(total time step 1-10) - ✅ Grasp success: Object lifted without
slipping - ✅ Placement accuracy: &lt;10mm from target center - ✅ No
collisions detected</p>
<p><strong>Concepts Demonstrated:</strong> - Computer vision (detection,
pose estimation) - Inverse kinematics - Motion planning (collision-free
trajectory) - Grasp planning (force closure) - State machine (task
sequencing) - Coordinate transforms (camera → robot frame)</p>
<p><strong>Demo Video Deliverable:</strong> 60-second video with screen
capture (RViz) + real robot</p>
<hr />
<h3 id="scenario-m2-multiple-objects-sequential-picking">Scenario M2:
Multiple Objects (Sequential Picking)</h3>
<p><strong>Objective:</strong> Pick 5 objects sequentially from
cluttered workspace</p>
<p><strong>Setup:</strong> - Objects: 5 colored cubes (red, blue, green,
yellow, black) randomly placed - Workspace: 600mm × 400mm area - Objects
may partially occlude each other</p>
<p><strong>Execution Steps:</strong> 1. Start system 2. For each object
(repeat 5 times): - Scan workspace - Detect all visible objects - Select
highest-confidence detection - Pick object - Place in designated zone
(indexed by color) 3. Report total time and success rate</p>
<p><strong>Success Criteria:</strong> - ✅ All 5 objects picked and
placed - ✅ Total cycle time: &lt;60 seconds - ✅ No “object not found”
errors - ✅ Objects placed in correct color-coded zones</p>
<p><strong>Concepts Demonstrated:</strong> - Multi-object detection -
Scene understanding (occlusion handling) - Task planning (object
prioritization) - Real-time replanning (workspace changes after each
pick)</p>
<p><strong>Demo Video Deliverable:</strong> 90-second time-lapse with
analytics overlay (objects remaining, cycle time)</p>
<hr />
<h3 id="scenario-m3-error-recovery-grasp-failure">Scenario M3: Error
Recovery (Grasp Failure)</h3>
<p><strong>Objective:</strong> Demonstrate graceful error recovery when
grasp fails</p>
<p><strong>Setup:</strong> - Object: Slippery cylinder (low friction,
challenging grasp) - Intentionally weak grasp force (50% of optimal)</p>
<p><strong>Execution Steps:</strong> 1. Start pick sequence 2. Gripper
grasps cylinder with insufficient force 3. During lift, F/T sensor
detects drop (force spike → 0N) 4. System detects grasp failure 5. Robot
returns to pre-grasp position 6. System displays error: “Grasp failed -
Retrying with increased force” 7. Retry grasp with 100% force 8.
Successfully lift and place object 9. Log failure event</p>
<p><strong>Success Criteria:</strong> - ✅ Grasp failure detected within
500ms - ✅ Retry succeeds on 2nd attempt - ✅ No objects damaged - ✅
Error logged with timestamp and cause</p>
<p><strong>Concepts Demonstrated:</strong> - Force/torque sensing (grasp
verification) - Error detection (sensor-based) - Adaptive control
(adjust grasp force) - State machine (error state → recovery state)</p>
<p><strong>Demo Video Deliverable:</strong> Split-screen (RViz + real
robot) showing failure and recovery</p>
<hr />
<h3 id="scenario-m4-calibration-wizard">Scenario M4: Calibration
Wizard</h3>
<p><strong>Objective:</strong> Demonstrate ease of camera-robot
calibration</p>
<p><strong>Setup:</strong> - Checkerboard pattern (8×6, 25mm squares) on
table - Camera uncalibrated (no prior hand-eye transform)</p>
<p><strong>Execution Steps:</strong> 1. Launch calibration wizard 2.
Wizard prompts: “Move robot to Position 1” (pre-defined joint angles) 3.
Operator confirms, wizard captures image 4. Repeat for Positions 2-5
(different robot poses) 5. Wizard computes hand-eye transformation
matrix 6. Validation: Place known object, system predicts position 7.
Wizard displays error: “Calibration error: 2.3mm (PASS)” 8. Save
calibration to <code>/config/camera_robot_tf.yaml</code></p>
<p><strong>Success Criteria:</strong> - ✅ Calibration completes in
&lt;5 minutes - ✅ Reprojection error &lt;5mm - ✅ Validation test
passes (object detected at correct position) - ✅ Calibration persists
across restarts</p>
<p><strong>Concepts Demonstrated:</strong> - Hand-eye calibration
(eye-to-hand configuration) - Coordinate frame transformations -
Usability (guided wizard for non-experts)</p>
<p><strong>Demo Video Deliverable:</strong> Screen capture of wizard UI,
narrated walkthrough</p>
<hr />
<h3 id="scenario-m5-safety-e-stop">Scenario M5: Safety E-Stop</h3>
<p><strong>Objective:</strong> Demonstrate emergency stop
functionality</p>
<p><strong>Setup:</strong> - Robot executing pick sequence (mid-motion)
- E-stop button accessible</p>
<p><strong>Execution Steps:</strong> 1. Start pick sequence 2. Robot
moving toward object (50% into trajectory) 3. Operator presses E-stop
button 4. Robot halts immediately, motors de-energized 5. System
displays: “EMERGENCY STOP - Press Reset to Continue” 6. Operator
releases E-stop, presses “Reset” 7. System prompts: “Return to Home?
(Y/N)” 8. Operator selects “Y”, robot returns to home position 9. System
ready for next pick</p>
<p><strong>Success Criteria:</strong> - ✅ Robot stops &lt;100ms after
E-stop pressed - ✅ No drift after stop (brakes engaged) - ✅ Cannot
restart without deliberate reset action - ✅ Event logged with
timestamp</p>
<p><strong>Concepts Demonstrated:</strong> - Safety-rated E-stop (SIL 2)
- Real-time control loop (fast response) - State machine (emergency
state)</p>
<p><strong>Demo Video Deliverable:</strong> Real-time video showing
E-stop activation and recovery</p>
<hr />
</div><div class="content-card"><h2 id="should-have-demo-scenarios">2. Should Have Demo Scenarios</h2>
<h3 id="scenario-s1-pose-variation-handling">Scenario S1: Pose Variation
Handling</h3>
<p><strong>Objective:</strong> Pick objects in arbitrary
orientations</p>
<p><strong>Setup:</strong> - Objects: 3 rectangular boxes (100mm × 50mm
× 30mm) placed at different angles - Orientations: 0°, 45°, 90° around
vertical axis</p>
<p><strong>Execution Steps:</strong> 1. For each object: - Detect
object, estimate 6DoF pose (x,y,z,roll,pitch,yaw) - Compute aligned
grasp (gripper oriented to object’s longest axis) - Pick and place 2.
Display pose estimates in RViz (TF frames)</p>
<p><strong>Success Criteria:</strong> - ✅ All 3 objects picked
regardless of orientation - ✅ Pose estimation error: &lt;5° rotation,
&lt;5mm position - ✅ Grasp aligned to object geometry</p>
<p><strong>Concepts Demonstrated:</strong> - 6DoF pose estimation (not
just centroid) - Grasp planning (orientation-aware) - TF
visualization</p>
<p><strong>Demo Video Deliverable:</strong> RViz visualization showing
estimated object frames overlaid on point cloud</p>
<hr />
<h3 id="scenario-s2-dynamic-conveyor-picking">Scenario S2: Dynamic
Conveyor Picking</h3>
<p><strong>Objective:</strong> Pick objects from a moving conveyor
belt</p>
<p><strong>Setup:</strong> - Conveyor belt moving at 0.1 m/s (constant
speed) - Objects: 4 cubes placed at 200mm intervals - Camera: Mounted
above belt, tracking motion</p>
<p><strong>Execution Steps:</strong> 1. Vision system tracks objects on
belt (optical flow / multi-frame tracking) 2. Predict object position at
time of grasp (t_grasp = t_detect + t_plan + t_move) 3. For each object:
- Estimate arrival time at pick zone - Pre-position robot (anticipatory
motion) - Pick object in motion (dynamic grasping) - Place in static
zone 4. Repeat until all objects picked</p>
<p><strong>Success Criteria:</strong> - ✅ All 4 objects picked without
stopping conveyor - ✅ Grasp success rate &gt;90% - ✅ No collisions
with conveyor</p>
<p><strong>Concepts Demonstrated:</strong> - Motion prediction (object
tracking) - Real-time planning (replanning during execution) -
Trajectory execution (moving target)</p>
<p><strong>Demo Video Deliverable:</strong> Side view + top view
(camera) showing synchronized pick</p>
<hr />
<h3 id="scenario-s3-workspace-customization">Scenario S3: Workspace
Customization</h3>
<p><strong>Objective:</strong> Demonstrate GUI for defining pick/place
zones</p>
<p><strong>Setup:</strong> - Blank workspace (table only) - RViz2 with
interactive markers</p>
<p><strong>Execution Steps:</strong> 1. Operator opens zone definition
tool in RViz 2. Draws pick zone (polygon tool, defines 2D boundary +
height range) - Pick zone: 400mm × 400mm, height: 0-200mm 3. Draws place
zone (300mm × 300mm, height: 50mm) 4. Draws exclusion zone (obstacle,
100mm × 100mm) 5. Save configuration to <code>zones.yaml</code> 6. Run
pick-place with new zones 7. System only picks from pick zone, places in
place zone, avoids exclusion</p>
<p><strong>Success Criteria:</strong> - ✅ Zones defined in &lt;2
minutes (intuitive UI) - ✅ Configuration saved and reloaded correctly -
✅ Robot respects zone boundaries (no picks outside pick zone)</p>
<p><strong>Concepts Demonstrated:</strong> - Planning scene management -
Collision objects (exclusion zones) - User-friendly configuration</p>
<p><strong>Demo Video Deliverable:</strong> Screen capture of zone
definition + robot respecting zones</p>
<hr />
<h3 id="scenario-s4-multi-gripper-support">Scenario S4: Multi-Gripper
Support</h3>
<p><strong>Objective:</strong> Swap gripper types and adapt grasp
strategy</p>
<p><strong>Setup:</strong> - Test with 2 gripper types: - Parallel jaw
(for cubes, boxes) - Suction (for flat, smooth objects like PCBs)</p>
<p><strong>Execution Steps:</strong> 1. <strong>Test 1: Parallel
Jaw</strong> - Object: Cube - Grasp: Pinch grasp from sides - Success:
Lifted with 20N force 2. Swap gripper (manual or auto-tool-changer) 3.
System detects gripper change, loads suction gripper config 4.
<strong>Test 2: Suction</strong> - Object: Flat PCB (100mm × 100mm) -
Grasp: Top-down suction - Success: Vacuum pressure confirms seal
(&gt;0.5 bar)</p>
<p><strong>Success Criteria:</strong> - ✅ Grasp planner adapts strategy
per gripper type - ✅ Both gripper types successfully pick objects - ✅
Gripper swap detected automatically (if using tool changer)</p>
<p><strong>Concepts Demonstrated:</strong> - End-effector modularity -
Grasp planning (type-specific algorithms) - Hardware abstraction</p>
<p><strong>Demo Video Deliverable:</strong> Side-by-side comparison of
parallel jaw vs suction grasps</p>
<hr />
<h3 id="scenario-s5-performance-dashboard">Scenario S5: Performance
Dashboard</h3>
<p><strong>Objective:</strong> Display real-time KPIs during
operation</p>
<p><strong>Setup:</strong> - Grafana dashboard open on separate monitor
- System running continuous pick-place loop (10 objects)</p>
<p><strong>Execution Steps:</strong> 1. Start pick-place loop 2.
Dashboard displays (real-time updates): - Current state (SCAN, PICK,
PLACE) - Objects processed (counter) - Cycle time (current, average,
p95) - Success rate (%) - Error log (scrolling list) - CPU/GPU
utilization graphs 3. Operator observes dashboard while robot works</p>
<p><strong>Success Criteria:</strong> - ✅ Dashboard updates with &lt;1
second latency - ✅ Metrics accurate (verified against ground truth) -
✅ Graphs show historical trends (last 10 minutes)</p>
<p><strong>Concepts Demonstrated:</strong> - Monitoring &amp;
observability - Prometheus + Grafana integration - Real-time data
visualization</p>
<p><strong>Demo Video Deliverable:</strong> Split-screen (robot +
dashboard) for 60 seconds</p>
<hr />
<h3 id="scenario-s6-simulation-validation">Scenario S6: Simulation
Validation</h3>
<p><strong>Objective:</strong> Run same workflow in simulation and real
hardware</p>
<p><strong>Setup:</strong> - Gazebo simulation with UR5e model, virtual
camera, physics engine - Identical object (cube) spawned in sim
workspace</p>
<p><strong>Execution Steps:</strong> 1. <strong>In Simulation:</strong>
- Launch: <code>ros2 launch vision_pickplace gazebo.launch.py</code> -
Run pick-place workflow - Record: cycle time, trajectory, grasp success
2. <strong>On Real Hardware:</strong> - Launch:
<code>ros2 launch vision_pickplace real_robot.launch.py</code> - Run
identical workflow - Record same metrics 3. Compare results (sim vs
real)</p>
<p><strong>Success Criteria:</strong> - ✅ Simulation runs without
errors - ✅ Cycle time difference &lt;20% (sim vs real) - ✅ Trajectory
similar (verified via joint plots) - ✅ Grasp success in both
environments</p>
<p><strong>Concepts Demonstrated:</strong> - Simulation fidelity
(Gazebo) - Sim-to-real transfer - Testing without hardware risk</p>
<p><strong>Demo Video Deliverable:</strong> Side-by-side video (Gazebo +
real robot) synchronized</p>
<hr />
</div><div class="content-card"><h2 id="may-have-demo-scenarios-advanced">3. May Have Demo Scenarios
(Advanced)</h2>
<h3 id="scenario-a1-bin-picking-with-pile-segmentation">Scenario A1: Bin
Picking with Pile Segmentation</h3>
<p><strong>Objective:</strong> Pick objects from a cluttered bin (random
pile)</p>
<p><strong>Setup:</strong> - Bin: 400mm × 400mm × 200mm deep - Objects:
20 cubes randomly dumped (overlapping, various orientations)</p>
<p><strong>Execution Steps:</strong> 1. Capture point cloud of bin 2.
Segment individual objects (clustering, region growing) 3. Identify
graspable objects (top layer, unoccluded) 4. Pick top object 5. Repeat
until bin empty (re-scan after each pick)</p>
<p><strong>Success Criteria:</strong> - ✅ All 20 objects picked (may
take multiple scans) - ✅ No collisions with bin walls - ✅ Success rate
&gt;85% (some failures expected with occlusions)</p>
<p><strong>Concepts Demonstrated:</strong> - 3D point cloud processing
(PCL) - Segmentation (clustering) - Iterative scene understanding</p>
<p><strong>Demo Video Deliverable:</strong> Time-lapse (accelerated 5x)
showing bin emptying</p>
<hr />
<h3 id="scenario-a2-collaborative-operation-human-in-loop">Scenario A2:
Collaborative Operation (Human-in-Loop)</h3>
<p><strong>Objective:</strong> Safely operate with human present in
workspace</p>
<p><strong>Setup:</strong> - Human (volunteer) standing near workspace -
Vision-based human detection (YOLO person class) - Safety zones defined
(inner: stop zone, outer: slow zone)</p>
<p><strong>Execution Steps:</strong> 1. Robot executing pick-place at
normal speed (100%) 2. Human approaches workspace (enters outer zone) 3.
System detects human, robot slows to 50% speed 4. Human enters inner
zone 5. Robot stops immediately (&lt;100ms) 6. System displays: “Human
detected - Waiting” 7. Human exits zone 8. After 2-second timeout, robot
resumes</p>
<p><strong>Success Criteria:</strong> - ✅ Human detected within 500ms -
✅ Robot stops before human contact - ✅ Speed reduction smooth (no
jerks) - ✅ System resumes automatically when safe</p>
<p><strong>Concepts Demonstrated:</strong> - Human-robot collaboration
(ISO/TS 15066) - Vision-based safety (redundant to laser scanners) -
Adaptive speed control</p>
<p><strong>Demo Video Deliverable:</strong> Wide-angle video showing
human and robot interaction</p>
<hr />
<h3 id="scenario-a3-ai-model-retraining-loop">Scenario A3: AI Model
Retraining Loop</h3>
<p><strong>Objective:</strong> Demonstrate model improvement from
production data</p>
<p><strong>Setup:</strong> - System collects 1000 pick images over 1
week (auto-logged) - Data scientist uses collected data to retrain
YOLO</p>
<p><strong>Execution Steps:</strong> 1. <strong>Data
Collection:</strong> - System logs all RGB-D images + labels (bounding
boxes) - Store in <code>/data/production_logs/</code> 2.
<strong>Retraining:</strong> - Load data into Label Studio (review
annotations) - Train YOLOv8 with fine-tuning (10 epochs) - Export to
ONNX 3. <strong>Deployment:</strong> - Upload new model to robot - A/B
test: 50% traffic to old model, 50% to new - Compare accuracy (new
model: 96% mAP, old: 92%) 4. <strong>Rollout:</strong> - New model
promoted to 100% traffic</p>
<p><strong>Success Criteria:</strong> - ✅ Data collection pipeline
works autonomously - ✅ Retraining improves accuracy (&gt;2% mAP gain) -
✅ A/B test infrastructure functional - ✅ Deployment seamless (no
downtime)</p>
<p><strong>Concepts Demonstrated:</strong> - ML Ops (training pipeline,
model registry) - Continuous improvement - A/B testing</p>
<p><strong>Demo Video Deliverable:</strong> Screencast of MLflow
experiments + before/after accuracy comparison</p>
<hr />
<h3 id="scenario-a4-multi-robot-coordination">Scenario A4: Multi-Robot
Coordination</h3>
<p><strong>Objective:</strong> Two robots working collaboratively in
shared workspace</p>
<p><strong>Setup:</strong> - 2× UR5e robots with shared workspace
(overlapping reach) - 10 objects to be sorted (5 per robot)</p>
<p><strong>Execution Steps:</strong> 1. Task allocator assigns objects
to robots based on proximity 2. Both robots execute pick-place
concurrently 3. Collision avoidance ensures no robot-robot collision 4.
If paths conflict, lower-priority robot yields (waits)</p>
<p><strong>Success Criteria:</strong> - ✅ All 10 objects sorted in
&lt;30 seconds (faster than single robot) - ✅ No collisions between
robots - ✅ Load balanced (5 objects per robot)</p>
<p><strong>Concepts Demonstrated:</strong> - Multi-robot planning -
Conflict resolution - Distributed task allocation</p>
<p><strong>Demo Video Deliverable:</strong> Overhead view showing both
robots working</p>
<hr />
<h3 id="scenario-a5-predictive-maintenance">Scenario A5: Predictive
Maintenance</h3>
<p><strong>Objective:</strong> Predict motor failure before it
happens</p>
<p><strong>Setup:</strong> - Logged data: motor temperatures, vibration,
cycle counts (simulated 6 months) - Trained ML model (LSTM) predicts
remaining useful life (RUL)</p>
<p><strong>Execution Steps:</strong> 1. System monitors motor health in
real-time 2. Model predicts: “Joint 3 RUL: 14 days” (based on
temperature trend) 3. Alert triggered: “Maintenance recommended for
Joint 3” 4. Maintenance scheduled (proactive, before failure) 5.
Post-maintenance: RUL resets to nominal</p>
<p><strong>Success Criteria:</strong> - ✅ Prediction accuracy &gt;80%
(validated on historical data) - ✅ Alert triggers 2 weeks before
predicted failure - ✅ No unexpected downtime</p>
<p><strong>Concepts Demonstrated:</strong> - Predictive analytics (ML
for maintenance) - Time-series forecasting (LSTM) - Proactive
maintenance</p>
<p><strong>Demo Video Deliverable:</strong> Grafana dashboard showing
RUL trends + alert</p>
<hr />
</div><div class="content-card"><h2 id="demo-scenario-summary-table">4. Demo Scenario Summary Table</h2>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 12%" />
<col style="width: 14%" />
<col style="width: 48%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Scenario</strong></th>
<th><strong>Category</strong></th>
<th><strong>Duration</strong></th>
<th><strong>Complexity</strong></th>
<th><strong>Key Concepts</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>M1</td>
<td>Must Have</td>
<td>60 sec</td>
<td>Low</td>
<td>Vision, IK, motion planning, grasp planning</td>
</tr>
<tr class="even">
<td>M2</td>
<td>Must Have</td>
<td>90 sec</td>
<td>Medium</td>
<td>Multi-object detection, task planning</td>
</tr>
<tr class="odd">
<td>M3</td>
<td>Must Have</td>
<td>90 sec</td>
<td>Medium</td>
<td>Error recovery, adaptive control, F/T sensing</td>
</tr>
<tr class="even">
<td>M4</td>
<td>Must Have</td>
<td>5 min</td>
<td>Medium</td>
<td>Hand-eye calibration, transforms</td>
</tr>
<tr class="odd">
<td>M5</td>
<td>Must Have</td>
<td>30 sec</td>
<td>Low</td>
<td>Safety, E-stop, state machine</td>
</tr>
<tr class="even">
<td>S1</td>
<td>Should Have</td>
<td>60 sec</td>
<td>Medium</td>
<td>6DoF pose estimation, oriented grasping</td>
</tr>
<tr class="odd">
<td>S2</td>
<td>Should Have</td>
<td>120 sec</td>
<td>High</td>
<td>Dynamic picking, motion prediction</td>
</tr>
<tr class="even">
<td>S3</td>
<td>Should Have</td>
<td>5 min</td>
<td>Low</td>
<td>Workspace customization, planning scene</td>
</tr>
<tr class="odd">
<td>S4</td>
<td>Should Have</td>
<td>90 sec</td>
<td>Medium</td>
<td>Multi-gripper support, hardware abstraction</td>
</tr>
<tr class="even">
<td>S5</td>
<td>Should Have</td>
<td>60 sec</td>
<td>Low</td>
<td>Monitoring, Grafana, observability</td>
</tr>
<tr class="odd">
<td>S6</td>
<td>Should Have</td>
<td>90 sec</td>
<td>Medium</td>
<td>Simulation, Gazebo, sim-to-real</td>
</tr>
<tr class="even">
<td>A1</td>
<td>May Have</td>
<td>5 min</td>
<td>High</td>
<td>Bin picking, point cloud segmentation</td>
</tr>
<tr class="odd">
<td>A2</td>
<td>May Have</td>
<td>90 sec</td>
<td>High</td>
<td>Human-robot collaboration, safety zones</td>
</tr>
<tr class="even">
<td>A3</td>
<td>May Have</td>
<td>10 min</td>
<td>High</td>
<td>ML Ops, model retraining, A/B testing</td>
</tr>
<tr class="odd">
<td>A4</td>
<td>May Have</td>
<td>60 sec</td>
<td>Very High</td>
<td>Multi-robot coordination, conflict resolution</td>
</tr>
<tr class="even">
<td>A5</td>
<td>May Have</td>
<td>5 min</td>
<td>High</td>
<td>Predictive maintenance, time-series forecasting</td>
</tr>
</tbody>
</table>
<p><strong>Total Demo Time:</strong> ~40 minutes (all scenarios)</p>
<hr />
</div><div class="content-card"><h2 id="demo-event-planning">5. Demo Event Planning</h2>
<h3 id="suggested-demo-flow-30-minute-presentation">5.1 Suggested Demo
Flow (30-minute presentation)</h3>
<p><strong>Segment 1: Introduction (5 min)</strong> - System overview
(slide deck) - Problem statement and value proposition - Live system
walkthrough (components: robot, camera, control PC)</p>
<p><strong>Segment 2: Core Functionality (15 min)</strong> -
<strong>M1:</strong> Basic pick-place (2 min live + narration) -
<strong>M2:</strong> Multiple objects (2 min) - <strong>M3:</strong>
Error recovery (2 min) - <strong>M4:</strong> Calibration wizard (5 min,
interactive) - <strong>M5:</strong> E-stop (1 min)</p>
<p><strong>Segment 3: Advanced Features (8 min)</strong> -
<strong>S1:</strong> Pose variation (video, 1 min) -
<strong>S2:</strong> Conveyor picking (video, 2 min) -
<strong>S5:</strong> Dashboard (live, 2 min) - <strong>A2:</strong>
Collaborative operation (video, 3 min)</p>
<p><strong>Segment 4: Q&amp;A (2 min)</strong></p>
<hr />
<h3 id="demo-checklist">5.2 Demo Checklist</h3>
<p><strong>Pre-Demo (1 hour before):</strong> - [ ] Power on robot,
camera, control PC - [ ] Verify network connectivity (ROS2 topics
visible) - [ ] Run health check (all sensors green) - [ ] Load demo
objects in workspace - [ ] Open dashboards (RViz, Grafana) on
presentation display - [ ] Test E-stop button</p>
<p><strong>During Demo:</strong> - [ ] Narrate each step clearly
(explain what system is doing) - [ ] Pause for questions between
scenarios - [ ] If failure occurs: explain error, show recovery (don’t
hide issues) - [ ] Point out key visualizations (bounding boxes,
trajectories, TF frames)</p>
<p><strong>Post-Demo:</strong> - [ ] Collect feedback (what impressed?
what needs improvement?) - [ ] Record demo metrics (cycle times,
accuracy, uptime) - [ ] Update demo scenarios based on feedback</p>
<hr />
</div><div class="content-card"><h2 id="demo-risk-mitigation">6. Demo Risk Mitigation</h2>
<table>
<colgroup>
<col style="width: 36%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Risk</strong></th>
<th><strong>Mitigation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Network failure (ROS2 comms)</td>
<td>Pre-check network, have backup recordings</td>
</tr>
<tr class="even">
<td>Camera not detecting object</td>
<td>Backup objects (high-contrast, known-good)</td>
</tr>
<tr class="odd">
<td>Grasp failure during demo</td>
<td>Tune gripper force beforehand, test 10× pre-demo</td>
</tr>
<tr class="even">
<td>Robot E-stop during demo</td>
<td>Test E-stop recovery procedure beforehand</td>
</tr>
<tr class="odd">
<td>Laptop/display issues</td>
<td>Backup laptop with pre-loaded software</td>
</tr>
<tr class="even">
<td>Power outage</td>
<td>UPS for critical systems</td>
</tr>
<tr class="odd">
<td>Software crash</td>
<td>Restart procedure documented, &lt;2 min recovery</td>
</tr>
</tbody>
</table>
<hr />
</div><div class="content-card"><h2 id="demo-metrics-to-collect">7. Demo Metrics to Collect</h2>
<table>
<colgroup>
<col style="width: 32%" />
<col style="width: 21%" />
<col style="width: 45%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Metric</strong></th>
<th><strong>Target</strong></th>
<th><strong>Measurement Method</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Cycle time (M1)</td>
<td>&lt;10 sec</td>
<td>Timestamp start to finish</td>
</tr>
<tr class="even">
<td>Multi-object throughput (M2)</td>
<td>&gt;5 picks/min</td>
<td>Total time / objects picked</td>
</tr>
<tr class="odd">
<td>Grasp success rate</td>
<td>&gt;95%</td>
<td>Successful picks / total attempts</td>
</tr>
<tr class="even">
<td>Calibration time (M4)</td>
<td>&lt;5 min</td>
<td>Stopwatch</td>
</tr>
<tr class="odd">
<td>E-stop response time (M5)</td>
<td>&lt;100 ms</td>
<td>Oscilloscope (button press → stop)</td>
</tr>
<tr class="even">
<td>Detection accuracy</td>
<td>&gt;95% mAP</td>
<td>Test on labeled dataset</td>
</tr>
<tr class="odd">
<td>Pose estimation error</td>
<td>&lt;5mm, &lt;5°</td>
<td>Ground truth from CMM</td>
</tr>
<tr class="even">
<td>Dashboard update latency</td>
<td>&lt;1 sec</td>
<td>Wall clock vs dashboard timestamp</td>
</tr>
</tbody>
</table>
<hr />
</div><div class="content-card"><h2 id="audience-specific-demo-variants">8. Audience-Specific Demo
Variants</h2>
<h3 id="for-technical-audience-engineers-researchers">For Technical
Audience (Engineers, Researchers)</h3>
<p><strong>Emphasize:</strong> - Architecture (show ROS2 node graph) -
Algorithms (explain YOLO, IK solver, RRT*) - Code walkthrough (brief,
show key modules) - Performance benchmarks (latency, throughput)</p>
<p><strong>Recommended Scenarios:</strong> M1, M2, M3, S1, S6, A3</p>
<hr />
<h3 id="for-business-audience-managers-executives">For Business Audience
(Managers, Executives)</h3>
<p><strong>Emphasize:</strong> - ROI (cycle time improvement, labor
savings) - Ease of use (M4 calibration wizard) - Reliability (M3 error
recovery, uptime metrics) - Dashboard (S5 KPI visualization)</p>
<p><strong>Recommended Scenarios:</strong> M1, M2, M3, M4, S5</p>
<hr />
<h3 id="for-safety-officers-regulators">For Safety Officers /
Regulators</h3>
<p><strong>Emphasize:</strong> - Safety compliance (ISO 10218, ISO/TS
15066) - E-stop functionality (M5) - Human detection (A2) - Audit logs
(immutable logs, retention)</p>
<p><strong>Recommended Scenarios:</strong> M5, A2, plus walk through
safety documentation</p>
<hr />
<h3 id="for-customers-end-users">For Customers / End Users</h3>
<p><strong>Emphasize:</strong> - Ease of deployment (M4 calibration) -
Reliability (M3 error recovery) - Performance (M2 throughput) - Support
(mention 24/7 support SLA)</p>
<p><strong>Recommended Scenarios:</strong> M1, M2, M3, M4, S3, S5</p>
<hr />
</div><div class="content-card"><h2 id="demo-video-production-guidelines">9. Demo Video Production
Guidelines</h2>
<p><strong>For Each Scenario:</strong> 1. <strong>Introduction Slide (5
sec):</strong> Scenario name, objective 2. <strong>Setup Overview (5
sec):</strong> Wide shot of workspace, label objects 3.
<strong>Execution (variable):</strong> Multiple camera angles: - Robot
close-up (gripper action) - Workspace overhead (full scene) - Screen
capture (RViz, dashboard) 4. <strong>Results (5 sec):</strong>
Success/fail indicators, metrics overlay 5. <strong>Conclusion Slide (3
sec):</strong> Key takeaway</p>
<p><strong>Technical Specs:</strong> - Resolution: 1080p (1920×1080) -
Frame rate: 30 fps - Format: MP4 (H.264 codec) - Audio: Narration (clear
voice, background music optional) - Graphics: Lower-third text overlay
with scenario name</p>
<hr />
</div><div class="content-card"><h2 id="conclusion">10. Conclusion</h2>
<p>This demo scenario collection provides: - <strong>5 Must Have
scenarios:</strong> Core functionality validation - <strong>6 Should
Have scenarios:</strong> Production-readiness features - <strong>5 May
Have scenarios:</strong> Advanced capabilities showcase - <strong>Total
demo time:</strong> 40 minutes (all scenarios) -
<strong>Audience-specific variants:</strong> Tailored for technical,
business, safety, customer audiences - <strong>Risk mitigation:</strong>
Backup plans for common failures - <strong>Metrics to collect:</strong>
Quantitative validation data</p>
<p><strong>Next Steps:</strong> 1. Implement Must Have scenarios first
(MVP) 2. Record high-quality demo videos for remote presentations 3.
Create interactive demo for trade shows (allow audience participation)
4. Gather feedback and refine scenarios iteratively</p>
<hr />
<p><strong>Document Status:</strong> ✅ Complete <strong>Last
Updated:</strong> 2025-10-18 <strong>Author:</strong> Product &amp; Demo
Team <strong>Review Status:</strong> Pending Review</p>

        </div>
        <div class="footer">
            <p><i class="fas fa-copyright"></i> 2025 VisionBot Project | Production-Ready Documentation</p>
            <p>Generated: 2025-10-19 15:54:45</p>
        </div>
        </body>
</html>
